{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9926addc-4bc8-4f34-b27a-dcc97e7ee84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TensorFlow completely blocked!\n",
      " PyTorch 2.6.0+cpu loaded successfully\n",
      "Import failed: TensorFlow disabled\n",
      "Proceeding with alternative pure PyTorch implementation...\n"
     ]
    }
   ],
   "source": [
    "# EMERGENCY FIX: Complete TensorFlow bypass for Windows\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Block TensorFlow completely\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "os.environ[\"USE_TENSORFLOW\"] = \"0\"  \n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# Mock TensorFlow to prevent any imports\n",
    "class MockTF:\n",
    "    def __getattr__(self, name): \n",
    "        raise ImportError(\"TensorFlow disabled\")\n",
    "\n",
    "# Block all TensorFlow imports\n",
    "tf_modules = [\n",
    "    'tensorflow', 'tensorflow.python', 'tensorflow.python.framework',\n",
    "    'tensorflow.python.pywrap_tensorflow', 'tensorflow.python._pywrap_tensorflow_internal'\n",
    "]\n",
    "\n",
    "for module in tf_modules:\n",
    "    sys.modules[module] = MockTF()\n",
    "\n",
    "print(\" TensorFlow completely blocked!\")\n",
    "\n",
    "# Now try importing transformers with only PyTorch backend\n",
    "try:\n",
    "    import torch\n",
    "    print(f\" PyTorch {torch.__version__} loaded successfully\")\n",
    "    \n",
    "    # Import transformers components individually to isolate issues\n",
    "    from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "    from transformers import Trainer, TrainingArguments, DefaultDataCollator\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    print(\" All imports successful! TensorFlow bypass worked!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Import failed: {e}\")\n",
    "    print(\"Proceeding with alternative pure PyTorch implementation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24597d7a-7b4b-4153-9896-4a4a621cb3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaning TensorFlow installations...\n",
      "✓ Removed tensorflow\n",
      "✓ Removed tensorflow-cpu\n",
      "✓ Removed tensorflow-gpu\n",
      "✓ Removed tf-nightly\n",
      "✓ Removed tf-estimator\n",
      "✓ Removed tensorboard\n",
      "✓ Removed tensorflow-io-gcs-filesystem\n",
      "✓ Removed tensorflow-estimator\n",
      "\n",
      " Installing PyTorch-only packages...\n",
      "\n",
      " PyTorch version: 2.6.0+cpu\n",
      " CUDA available: False\n",
      " Installation complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Clean installation approach to avoid TensorFlow conflicts\n",
    "# This cell ensures a clean PyTorch-only environment\n",
    "\n",
    "# First, completely remove any TensorFlow installations\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\" Cleaning TensorFlow installations...\")\n",
    "tf_packages = [\n",
    "    'tensorflow', 'tensorflow-cpu', 'tensorflow-gpu', \n",
    "    'tf-nightly', 'tf-estimator', 'tensorboard',\n",
    "    'tensorflow-io-gcs-filesystem', 'tensorflow-estimator'\n",
    "]\n",
    "\n",
    "for package in tf_packages:\n",
    "    try:\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'uninstall', package, '-y'], \n",
    "                      capture_output=True, check=False)\n",
    "        print(f\"✓ Removed {package}\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"\\n Installing PyTorch-only packages...\")\n",
    "\n",
    "# Install PyTorch first (CPU version for compatibility)\n",
    "subprocess.run([\n",
    "    sys.executable, '-m', 'pip', 'install', \n",
    "    'torch', 'torchvision', 'torchaudio', \n",
    "    '--index-url', 'https://download.pytorch.org/whl/cpu'\n",
    "], check=True)\n",
    "\n",
    "# Install transformers with specific PyTorch backend\n",
    "subprocess.run([\n",
    "    sys.executable, '-m', 'pip', 'install', \n",
    "    'transformers[torch]', 'datasets', 'accelerate'\n",
    "], check=True)\n",
    "\n",
    "# Verify PyTorch installation\n",
    "import torch\n",
    "print(f\"\\n PyTorch version: {torch.__version__}\")\n",
    "print(f\" CUDA available: {torch.cuda.is_available()}\")\n",
    "print(\" Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3750f794-c150-454c-9f81-9d04908dcbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ AutoTokenizer imported successfully\n",
      "✓ AutoModelForQuestionAnswering imported successfully\n",
      "✓ TrainingArguments imported successfully\n",
      "✗ Failed to import Trainer: TensorFlow is disabled\n",
      "✓ DefaultDataCollator imported successfully\n",
      "\n",
      " Setup complete! Using device: cpu\n",
      "PyTorch version: 2.6.0+cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import all necessary libraries with TensorFlow bypass\n",
    "# This approach prevents TensorFlow from being imported by transformers\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# CRITICAL: Set environment variables to disable TensorFlow before any imports\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "\n",
    "# Mock tensorflow module to prevent import errors\n",
    "class MockTensorFlow:\n",
    "    def __getattr__(self, name):\n",
    "        raise ImportError(\"TensorFlow is disabled\")\n",
    "\n",
    "# Insert mock tensorflow into sys.modules before importing transformers\n",
    "sys.modules['tensorflow'] = MockTensorFlow()\n",
    "sys.modules['tensorflow.python'] = MockTensorFlow()\n",
    "sys.modules['tensorflow.python.framework'] = MockTensorFlow()\n",
    "\n",
    "# Now import PyTorch and other required libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import transformers components one by one to catch any TF dependencies\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    print(\"✓ AutoTokenizer imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Failed to import AutoTokenizer: {e}\")\n",
    "    \n",
    "try:\n",
    "    from transformers import AutoModelForQuestionAnswering\n",
    "    print(\"✓ AutoModelForQuestionAnswering imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Failed to import AutoModelForQuestionAnswering: {e}\")\n",
    "\n",
    "try:\n",
    "    from transformers import TrainingArguments\n",
    "    print(\"✓ TrainingArguments imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Failed to import TrainingArguments: {e}\")\n",
    "\n",
    "try:\n",
    "    from transformers import Trainer\n",
    "    print(\"✓ Trainer imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Failed to import Trainer: {e}\")\n",
    "\n",
    "try:\n",
    "    from transformers import DefaultDataCollator\n",
    "    print(\"✓ DefaultDataCollator imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Failed to import DefaultDataCollator: {e}\")\n",
    "\n",
    "# Import other required libraries\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n Setup complete! Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2e478a6-bd21-4c56-b3c0-ac36df529ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca7f0a1d1524c569066f657d122ed13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\siddu\\.cache\\huggingface\\hub\\datasets--squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55aeedd882f4a139c5ae75b9805b83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3024a7fc09dd43abbfb70db3dc725d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a845b074ee5d424997355eb309dfdf16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e072b157eccb49949bb5a35e0b6f8a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 200\n",
      "Evaluation samples: 50\n",
      "\n",
      "Sample training example:\n",
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper sta...\n",
      "Answer: {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load SQuAD v1.1 dataset and create small subset for fast prototyping\n",
    "# We'll use only 200 training samples and 50 evaluation samples\n",
    "\n",
    "# Load the full SQuAD dataset\n",
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "# Create small subsets for quick training and evaluation\n",
    "train_dataset = dataset[\"train\"].select(range(200))  # First 200 training samples\n",
    "eval_dataset = dataset[\"validation\"].select(range(50))  # First 50 validation samples\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Evaluation samples: {len(eval_dataset)}\")\n",
    "\n",
    "# Display a sample to understand the data structure\n",
    "print(\"\\nSample training example:\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"Question: {sample['question']}\")\n",
    "print(f\"Context: {sample['context'][:200]}...\")\n",
    "print(f\"Answer: {sample['answers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c732414-9a7d-4f73-8910-ce22fe6dac68",
   "metadata": {},
   "source": [
    "## Load DistilBERT Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fb94c7d-3c19-4c63-bc5e-69b1e45d5ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae1e1ab845c4fe4a60fd96aa7c89936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\siddu\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa96ff1e5bb42c3bb04484e5b83e480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/451 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33e56278719405e87dd7148241fe1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d553ca6d21404fbb97c6c6becab9f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e391b042c04fc09c2e82803067df06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: distilbert-base-uncased-distilled-squad\n",
      "Tokenizer vocab size: 30522\n",
      "Model parameters: 66,364,418\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Load DistilBERT tokenizer and model for Question Answering\n",
    "# DistilBERT is chosen for its speed and efficiency while maintaining good performance\n",
    "\n",
    "model_name = \"distilbert-base-uncased-distilled-squad\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model for question answering\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Move model to appropriate device\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fad2306-4276-46fa-92ba-9a4779cc64e6",
   "metadata": {},
   "source": [
    "## Define Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e15fba9c-72b3-4bb5-b5ce-44022dee1315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization function defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Define preprocessing function to tokenize question-context pairs\n",
    "# This function handles the crucial task of mapping answer spans to token positions\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize questions and contexts, and map answer spans to token positions.\n",
    "    This is crucial for training the model to predict start and end positions.\n",
    "    \"\"\"\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    contexts = examples[\"context\"]\n",
    "    \n",
    "    # Tokenize with truncation and padding\n",
    "    tokenized_examples = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=384,  # Standard length for SQuAD\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        stride=128\n",
    "    )\n",
    "    \n",
    "    # Initialize lists for start and end positions\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    # Map answer spans to token positions\n",
    "    for i, offsets in enumerate(tokenized_examples[\"offset_mapping\"]):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        \n",
    "        # Get the sequence that corresponds to our example\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        \n",
    "        # Find the start and end of the context\n",
    "        context_start = sequence_ids.index(1) if 1 in sequence_ids else None\n",
    "        context_end = len(sequence_ids) - 1 - sequence_ids[::-1].index(1) if 1 in sequence_ids else None\n",
    "        \n",
    "        # If no context, set answer as impossible\n",
    "        if context_start is None:\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "            continue\n",
    "            \n",
    "        # Get the answer from the original example\n",
    "        # Handle the case where we might have overflow tokens\n",
    "        sample_index = tokenized_examples[\"overflow_to_sample_mapping\"][i]\n",
    "        answer = examples[\"answers\"][sample_index]\n",
    "        \n",
    "        if len(answer[\"answer_start\"]) == 0:\n",
    "            # No answer case\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "        else:\n",
    "            # Find answer start and end positions\n",
    "            start_char = answer[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answer[\"text\"][0])\n",
    "            \n",
    "            # Find token positions corresponding to character positions\n",
    "            token_start_index = context_start\n",
    "            while token_start_index <= context_end and offsets[token_start_index][0] <= start_char:\n",
    "                token_start_index += 1\n",
    "            start_positions.append(token_start_index - 1)\n",
    "            \n",
    "            token_end_index = context_end\n",
    "            while token_end_index >= context_start and offsets[token_end_index][1] >= end_char:\n",
    "                token_end_index -= 1\n",
    "            end_positions.append(token_end_index + 1)\n",
    "    \n",
    "    tokenized_examples[\"start_positions\"] = start_positions\n",
    "    tokenized_examples[\"end_positions\"] = end_positions\n",
    "    \n",
    "    return tokenized_examples\n",
    "\n",
    "print(\"Tokenization function defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899f616b-4cdf-4df8-9775-c76978dde389",
   "metadata": {},
   "source": [
    "## Apply Preprocession to Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6da20dac-200f-4a6e-b0de-b3325d3b6f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e471db71fec4545af6f5a08d39a9bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing evaluation dataset...\n",
      "Tokenized training samples: 200\n",
      "Tokenized evaluation samples: 50\n",
      "\n",
      "Tokenized sample structure:\n",
      "dict_keys(['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping', 'start_positions', 'end_positions'])\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Apply the preprocessing function to both training and evaluation datasets\n",
    "# This creates the final tokenized datasets ready for training\n",
    "\n",
    "print(\"Preprocessing training dataset...\")\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"Preprocessing evaluation dataset...\")\n",
    "tokenized_eval_dataset = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"Tokenized training samples: {len(tokenized_train_dataset)}\")\n",
    "print(f\"Tokenized evaluation samples: {len(tokenized_eval_dataset)}\")\n",
    "\n",
    "# Verify the structure of tokenized data\n",
    "print(\"\\nTokenized sample structure:\")\n",
    "print(tokenized_train_dataset[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0845eb55-0e05-41bb-9946-bde79f495a0b",
   "metadata": {},
   "source": [
    "## Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8a8cb03-5af4-47f3-9e19-872cb5686b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.54.1\n",
      " Training arguments configured for fast prototyping!\n",
      " Total training steps: 75\n",
      " Evaluation strategy: IntervalStrategy.EPOCH\n",
      " Output directory: ./results\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Set up training arguments optimized for quick prototyping\n",
    "# Small batch size and few epochs for fast iteration during development\n",
    "\n",
    "# Check transformers version to use correct parameter names\n",
    "import transformers\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "\n",
    "# Use updated parameter names for newer transformers versions\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",              # Directory to save model checkpoints\n",
    "    eval_strategy=\"epoch\",               # Updated parameter name (was evaluation_strategy)\n",
    "    learning_rate=3e-5,                  # Standard learning rate for BERT-like models\n",
    "    per_device_train_batch_size=8,       # Small batch size for memory efficiency\n",
    "    per_device_eval_batch_size=8,        # Small batch size for evaluation\n",
    "    num_train_epochs=3,                  # Few epochs for quick training\n",
    "    weight_decay=0.01,                   # L2 regularization\n",
    "    logging_dir=\"./logs\",                # Directory for storing logs\n",
    "    logging_steps=10,                    # Log every 10 steps\n",
    "    save_strategy=\"epoch\",               # Save model at the end of each epoch\n",
    "    load_best_model_at_end=True,         # Load the best model when training ends\n",
    "    metric_for_best_model=\"eval_loss\",   # Use evaluation loss to determine best model\n",
    "    greater_is_better=False,             # Lower loss is better\n",
    "    warmup_steps=10,                     # Number of warmup steps for learning rate scheduler\n",
    "    report_to=\"none\",                    # Disable wandb/tensorboard logging (updated from None)\n",
    "    dataloader_pin_memory=False,         # Disable pin memory to avoid potential issues\n",
    "    push_to_hub=False,                   # Don't push to Hugging Face Hub\n",
    "    use_cpu=device.type == \"cpu\",        # Use CPU if no CUDA available\n",
    ")\n",
    "\n",
    "print(\" Training arguments configured for fast prototyping!\")\n",
    "print(f\" Total training steps: {len(tokenized_train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs}\")\n",
    "print(f\" Evaluation strategy: {training_args.eval_strategy}\")\n",
    "print(f\" Output directory: {training_args.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75b56ff-835b-4361-9c5b-88b7d1c04c04",
   "metadata": {},
   "source": [
    "## Setup Trainer with Data Collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "639de26e-990d-4e76-8d36-e40faa1f353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Failed to import Trainer: TensorFlow is disabled\n",
      " Trying alternative import...\n",
      " Still failed to import Trainer: TensorFlow is disabled\n",
      " Using manual training loop instead...\n",
      "\n",
      " Checking required components:\n",
      " model: Available\n",
      " training_args: Available\n",
      " tokenized_train_dataset: Available\n",
      " tokenized_eval_dataset: Available\n",
      " tokenizer: Available\n",
      "\n",
      " All components available!\n",
      " DefaultDataCollator initialized\n",
      "\n",
      " Trainer creation failed: name 'Trainer' is not defined\n",
      "\n",
      " Creating manual training loop as fallback...\n",
      " Manual training setup complete!\n",
      " Train batches: 25\n",
      " Eval batches: 7\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Initialize the Trainer with PyTorch-compatible components\n",
    "# First, let's ensure all components are properly imported\n",
    "\n",
    "# Re-import components if needed (handles any import issues)\n",
    "try:\n",
    "    from transformers import Trainer\n",
    "    print(\" Trainer imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\" Failed to import Trainer: {e}\")\n",
    "    print(\" Trying alternative import...\")\n",
    "    \n",
    "    # Alternative import approach\n",
    "    import sys\n",
    "    import importlib\n",
    "    \n",
    "    # Force reload transformers module\n",
    "    if 'transformers' in sys.modules:\n",
    "        importlib.reload(sys.modules['transformers'])\n",
    "    \n",
    "    try:\n",
    "        from transformers import Trainer\n",
    "        print(\" Trainer imported successfully on retry\")\n",
    "    except ImportError as e:\n",
    "        print(f\" Still failed to import Trainer: {e}\")\n",
    "        print(\" Using manual training loop instead...\")\n",
    "\n",
    "# Verify all required components are available\n",
    "required_components = {\n",
    "    'model': 'model',\n",
    "    'training_args': 'training_args', \n",
    "    'tokenized_train_dataset': 'tokenized_train_dataset',\n",
    "    'tokenized_eval_dataset': 'tokenized_eval_dataset',\n",
    "    'tokenizer': 'tokenizer'\n",
    "}\n",
    "\n",
    "print(\"\\n Checking required components:\")\n",
    "missing_components = []\n",
    "for name, var_name in required_components.items():\n",
    "    try:\n",
    "        eval(var_name)\n",
    "        print(f\" {name}: Available\")\n",
    "    except NameError:\n",
    "        print(f\" {name}: Missing\")\n",
    "        missing_components.append(name)\n",
    "\n",
    "if missing_components:\n",
    "    print(f\"\\n  Missing components: {missing_components}\")\n",
    "    print(\"Please run the previous cells first!\")\n",
    "else:\n",
    "    print(\"\\n All components available!\")\n",
    "\n",
    "# Initialize the data collator (PyTorch-compatible)\n",
    "try:\n",
    "    from transformers import DefaultDataCollator\n",
    "    data_collator = DefaultDataCollator(return_tensors=\"pt\")\n",
    "    print(\" DefaultDataCollator initialized\")\n",
    "except ImportError:\n",
    "    # Manual data collator as fallback\n",
    "    print(\" Using manual data collator...\")\n",
    "    \n",
    "    def manual_data_collator(features):\n",
    "        \"\"\"Manual data collator for PyTorch tensors\"\"\"\n",
    "        batch = {}\n",
    "        first = features[0]\n",
    "        \n",
    "        for key in first.keys():\n",
    "            if key in ['input_ids', 'attention_mask', 'start_positions', 'end_positions']:\n",
    "                batch[key] = torch.stack([torch.tensor(f[key]) for f in features])\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    data_collator = manual_data_collator\n",
    "    print(\" Manual data collator created\")\n",
    "\n",
    "# Create the Trainer (with error handling)\n",
    "try:\n",
    "    trainer = Trainer(\n",
    "        model=model,                           # Our DistilBERT model\n",
    "        args=training_args,                    # Training configuration\n",
    "        train_dataset=tokenized_train_dataset, # Tokenized training data\n",
    "        eval_dataset=tokenized_eval_dataset,   # Tokenized evaluation data\n",
    "        data_collator=data_collator,           # PyTorch data collator\n",
    "        tokenizer=tokenizer,                   # Tokenizer for saving\n",
    "    )\n",
    "    \n",
    "    print(\"\\n Trainer initialized successfully!\")\n",
    "    print(\" Ready to start training...\")\n",
    "    \n",
    "except NameError as e:\n",
    "    print(f\"\\n Trainer creation failed: {e}\")\n",
    "    print(\"\\n Creating manual training loop as fallback...\")\n",
    "    \n",
    "    # Manual training setup\n",
    "    from torch.optim import AdamW\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataloader = DataLoader(\n",
    "        tokenized_train_dataset, \n",
    "        batch_size=training_args.per_device_train_batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    \n",
    "    eval_dataloader = DataLoader(\n",
    "        tokenized_eval_dataset,\n",
    "        batch_size=training_args.per_device_eval_batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)\n",
    "    \n",
    "    print(\" Manual training setup complete!\")\n",
    "    print(f\" Train batches: {len(train_dataloader)}\")\n",
    "    print(f\" Eval batches: {len(eval_dataloader)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n Unexpected error: {e}\")\n",
    "    print(\"Please check previous cells and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040d1dea-9e76-4ea6-828d-4136cccbe059",
   "metadata": {},
   "source": [
    "## Train the model (with fallback options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0581395-c533-4c66-b481-392e11884ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting training process...\n",
      "==================================================\n",
      " Using manual training loop...\n",
      "\n",
      "==================================================\n",
      "  MANUAL TRAINING LOOP\n",
      "==================================================\n",
      " Training for 3 epochs...\n",
      "\n",
      " Epoch 1/3\n",
      "  Step 10, Batch 10/25, Loss: 0.0251\n",
      "  Step 20, Batch 20/25, Loss: 0.0402\n",
      " Epoch 1 completed. Average loss: 0.0438\n",
      "\n",
      " Epoch 2/3\n",
      "  Step 35, Batch 10/25, Loss: 0.0343\n",
      "  Step 45, Batch 20/25, Loss: 0.0454\n",
      " Epoch 2 completed. Average loss: 0.0386\n",
      "\n",
      " Epoch 3/3\n",
      "  Step 60, Batch 10/25, Loss: 0.0227\n",
      "  Step 70, Batch 20/25, Loss: 0.0192\n",
      " Epoch 3 completed. Average loss: 0.0196\n",
      " Running evaluation...\n",
      " Evaluation loss: 1.9928\n",
      "\n",
      " Manual training completed!\n",
      " Final average loss: 0.0340\n",
      " Saving model...\n",
      " Model saved to './final_model'\n",
      "\n",
      "==================================================\n",
      " TRAINING PROCESS COMPLETED!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Train the model with multiple approaches\n",
    "# This cell handles both Trainer-based and manual training\n",
    "\n",
    "import time\n",
    "\n",
    "print(\" Starting training process...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if Trainer was successfully created\n",
    "if 'trainer' in locals():\n",
    "    print(\" Using Hugging Face Trainer for training...\")\n",
    "    \n",
    "    try:\n",
    "        # Start training with Trainer\n",
    "        print(\" Training started...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        \n",
    "        print(f\"\\n Training completed in {training_time:.2f} seconds!\")\n",
    "        \n",
    "        # Save the final model\n",
    "        trainer.save_model(\"./final_model\")\n",
    "        tokenizer.save_pretrained(\"./final_model\")\n",
    "        print(\" Model and tokenizer saved to './final_model'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Training with Trainer failed: {e}\")\n",
    "        print(\" Switching to manual training...\")\n",
    "        use_manual_training = True\n",
    "\n",
    "else:\n",
    "    print(\" Using manual training loop...\")\n",
    "    use_manual_training = True\n",
    "\n",
    "# Manual training loop (fallback)\n",
    "if 'use_manual_training' in locals() or 'trainer' not in locals():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"  MANUAL TRAINING LOOP\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    step = 0\n",
    "    \n",
    "    print(f\" Training for {training_args.num_train_epochs} epochs...\")\n",
    "    \n",
    "    for epoch in range(int(training_args.num_train_epochs)):\n",
    "        print(f\"\\n Epoch {epoch + 1}/{training_args.num_train_epochs}\")\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch.get('attention_mask', None)\n",
    "            )\n",
    "            \n",
    "            # Calculate loss (simplified QA loss)\n",
    "            start_positions = batch.get('start_positions', torch.zeros(batch['input_ids'].size(0)).long().to(device))\n",
    "            end_positions = batch.get('end_positions', torch.zeros(batch['input_ids'].size(0)).long().to(device))\n",
    "            \n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            start_loss = loss_fct(outputs['start_logits'], start_positions)\n",
    "            end_loss = loss_fct(outputs['end_logits'], end_positions)\n",
    "            loss = (start_loss + end_loss) / 2\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track loss\n",
    "            epoch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            step += 1\n",
    "            \n",
    "            # Log progress\n",
    "            if (batch_idx + 1) % training_args.logging_steps == 0:\n",
    "                avg_loss = epoch_loss / (batch_idx + 1)\n",
    "                print(f\"  Step {step}, Batch {batch_idx + 1}/{len(train_dataloader)}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # End of epoch\n",
    "        avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "        print(f\" Epoch {epoch + 1} completed. Average loss: {avg_epoch_loss:.4f}\")\n",
    "        \n",
    "        # Simple evaluation\n",
    "        if epoch == training_args.num_train_epochs - 1:  # Evaluate on last epoch\n",
    "            print(\" Running evaluation...\")\n",
    "            model.eval()\n",
    "            eval_loss = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for eval_batch in eval_dataloader:\n",
    "                    eval_batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in eval_batch.items()}\n",
    "                    \n",
    "                    eval_outputs = model(\n",
    "                        input_ids=eval_batch['input_ids'],\n",
    "                        attention_mask=eval_batch.get('attention_mask', None)\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate evaluation loss\n",
    "                    eval_start_positions = eval_batch.get('start_positions', torch.zeros(eval_batch['input_ids'].size(0)).long().to(device))\n",
    "                    eval_end_positions = eval_batch.get('end_positions', torch.zeros(eval_batch['input_ids'].size(0)).long().to(device))\n",
    "                    \n",
    "                    eval_start_loss = loss_fct(eval_outputs['start_logits'], eval_start_positions)\n",
    "                    eval_end_loss = loss_fct(eval_outputs['end_logits'], eval_end_positions)\n",
    "                    eval_batch_loss = (eval_start_loss + eval_end_loss) / 2\n",
    "                    \n",
    "                    eval_loss += eval_batch_loss.item()\n",
    "            \n",
    "            avg_eval_loss = eval_loss / len(eval_dataloader)\n",
    "            print(f\" Evaluation loss: {avg_eval_loss:.4f}\")\n",
    "            \n",
    "            model.train()  # Back to training mode\n",
    "    \n",
    "    print(f\"\\n Manual training completed!\")\n",
    "    print(f\" Final average loss: {total_loss / step:.4f}\")\n",
    "    \n",
    "    # Save model manually\n",
    "    print(\" Saving model...\")\n",
    "    torch.save(model.state_dict(), \"./final_model/pytorch_model.bin\")\n",
    "    tokenizer.save_pretrained(\"./final_model\")\n",
    "    \n",
    "    # Save model config\n",
    "    import json\n",
    "    config = {\n",
    "        \"vocab_size\": model.embedding.num_embeddings if hasattr(model, 'embedding') else 30522,\n",
    "        \"hidden_size\": 768,\n",
    "        \"model_type\": \"custom_qa_model\"\n",
    "    }\n",
    "    with open(\"./final_model/config.json\", \"w\") as f:\n",
    "        json.dump(config, f)\n",
    "    \n",
    "    print(\" Model saved to './final_model'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\" TRAINING PROCESS COMPLETED!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db15ea2-2d0a-4c83-b95c-a1b0fc3f4868",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7cc0f79-21f3-4435-b7bc-c445003190dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Cell 10: Evaluate the trained model and compute metrics\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# We'll compute basic metrics and also implement simple EM and F1 scores\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m(\n\u001b[0;32m      5\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,                           \u001b[38;5;66;03m# Our DistilBERT model\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                    \u001b[38;5;66;03m# Training configuration\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_train_dataset, \u001b[38;5;66;03m# Tokenized training data\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_eval_dataset,   \u001b[38;5;66;03m# Tokenized evaluation data\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,           \u001b[38;5;66;03m# PyTorch data collator\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,                   \u001b[38;5;66;03m# Tokenizer for saving\u001b[39;00m\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[0;32m     14\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 10: Evaluate the trained model and compute metrics\n",
    "# We'll compute basic metrics and also implement simple EM and F1 scores\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# Simple evaluation functions for exact match and F1 score\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    \n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    \n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    \n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    \n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    \"\"\"Compute exact match score between prediction and ground truth.\"\"\"\n",
    "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    \"\"\"Compute F1 score between prediction and ground truth.\"\"\"\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    \n",
    "    if len(prediction_tokens) == 0 or len(ground_truth_tokens) == 0:\n",
    "        return int(prediction_tokens == ground_truth_tokens)\n",
    "    \n",
    "    common_tokens = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common_tokens.values())\n",
    "    \n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return f1\n",
    "\n",
    "print(\"\\nCustom evaluation metrics computed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a07000e-bf5f-4504-90ca-6974c171803f",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "039b7456-2f99-48da-bf9d-ca161b2fa520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INFERENCE EXAMPLE ===\n",
      "Context: \n",
      "The Amazon rainforest, also known as Amazonia, is a moist broadleaf tropical rainforest \n",
      "in the Amazon biome that covers most of the Amazon basin of South America. This basin \n",
      "encompasses 7,000,000 s...\n",
      "Question: How large is the Amazon basin?\n",
      "Predicted Answer: 7, 000, 000 square kilometers\n",
      "Confidence: 0.9754\n",
      "\n",
      "Question 2: How many nations are included in the Amazon region?\n",
      "Predicted Answer: nine\n",
      "Confidence: 0.9900\n",
      "\n",
      "=== MODEL READY FOR USE ===\n",
      "You can now use the predict_answer() function with any question and context!\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Inference cell - test the trained model with custom questions\n",
    "# This demonstrates how to use the model for predictions on new data\n",
    "\n",
    "def predict_answer(question, context):\n",
    "    \"\"\"\n",
    "    Predict answer for a given question and context using the trained model.\n",
    "    \"\"\"\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(\n",
    "        question, \n",
    "        context, \n",
    "        return_tensors=\"pt\",\n",
    "        max_length=384,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        start_logits = outputs.start_logits\n",
    "        end_logits = outputs.end_logits\n",
    "    \n",
    "    # Get the most likely beginning and end of answer\n",
    "    start_index = torch.argmax(start_logits)\n",
    "    end_index = torch.argmax(end_logits)\n",
    "    \n",
    "    # Convert token indices to answer text\n",
    "    input_ids = inputs[\"input_ids\"][0]\n",
    "    answer_tokens = input_ids[start_index:end_index + 1]\n",
    "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    # Get confidence scores\n",
    "    start_score = torch.softmax(start_logits, dim=1)[0][start_index].item()\n",
    "    end_score = torch.softmax(end_logits, dim=1)[0][end_index].item()\n",
    "    confidence = (start_score + end_score) / 2\n",
    "    \n",
    "    return answer, confidence\n",
    "\n",
    "# Test with a custom example\n",
    "test_context = \"\"\"\n",
    "The Amazon rainforest, also known as Amazonia, is a moist broadleaf tropical rainforest \n",
    "in the Amazon biome that covers most of the Amazon basin of South America. This basin \n",
    "encompasses 7,000,000 square kilometers, of which 5,500,000 square kilometers are covered \n",
    "by the rainforest. This region includes territory belonging to nine nations.\n",
    "\"\"\"\n",
    "\n",
    "test_question = \"How large is the Amazon basin?\"\n",
    "\n",
    "# Make prediction\n",
    "predicted_answer, confidence = predict_answer(test_question, test_context)\n",
    "\n",
    "print(\"=== INFERENCE EXAMPLE ===\")\n",
    "print(f\"Context: {test_context[:200]}...\")\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"Predicted Answer: {predicted_answer}\")\n",
    "print(f\"Confidence: {confidence:.4f}\")\n",
    "\n",
    "# Test with another example\n",
    "test_question_2 = \"How many nations are included in the Amazon region?\"\n",
    "predicted_answer_2, confidence_2 = predict_answer(test_question_2, test_context)\n",
    "\n",
    "print(f\"\\nQuestion 2: {test_question_2}\")\n",
    "print(f\"Predicted Answer: {predicted_answer_2}\")\n",
    "print(f\"Confidence: {confidence_2:.4f}\")\n",
    "\n",
    "print(\"\\n=== MODEL READY FOR USE ===\")\n",
    "print(\"You can now use the predict_answer() function with any question and context!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
